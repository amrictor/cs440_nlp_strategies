{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class character_model:\n",
    "    def __init__(self, filename, numberLines=20, minimumOccurCount=1):\n",
    "        self.characters = []\n",
    "        self.content = []\n",
    "        self.numberLines = numberLines\n",
    "        self.minimumOccurCount = minimumOccurCount\n",
    "        self.clean_input_document(filename)\n",
    "        self.generate_character_order()\n",
    "        \n",
    "    def clean_input_document(self, filename):\n",
    "        characters = []\n",
    "        \n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                line = line.lower().split(\"|\")\n",
    "                if len(line) == 2:\n",
    "                    characters.append(\"action\")\n",
    "                elif len(line) == 3:\n",
    "                    characters.append(line[1].strip())\n",
    "\n",
    "        #REMOVE CHARACTERS WHO DONT MEET MINIMUM\n",
    "        lowCountChar = [char for char, occur in Counter(characters).items() \\\n",
    "                                        if occur < self.minimumOccurCount]\n",
    "        characters = [char for char in characters if char not in lowCountChar]\n",
    "        self.characters = characters\n",
    "        self.create_character_model()\n",
    "    \n",
    "    def create_character_model(self):  \n",
    "        #FIRST, FOR EACH CHARACTER, RECORD WHO FOLLOWS THEM IN THE SCRIPT\n",
    "        character_follow = defaultdict(list)\n",
    "        for i in range(1,len(self.characters)):\n",
    "            if self.characters[i-1] == self.characters[i]:\n",
    "                continue\n",
    "            character_follow[self.characters[i-1]].append(self.characters[i])\n",
    "\n",
    "        #NEXT, COUNT THE OCCURANCES FOR EACH PERSON WHO FOLLOWS THEM IN THE SCRIPT\n",
    "        character_follow_count = {}\n",
    "        for k, v in character_follow.items():\n",
    "            character_follow_count[k] = ( Counter(v), len(v) )\n",
    "\n",
    "        #THEN, CREATE TWO LISTS: CHARACTERS AND THEIR CHANCE OF BEING SELECTED\n",
    "        #EX -> {'KK':3, 'TT':2} => ['KK', 'TT'], [3/5, 2/5]\n",
    "        char_probs = {}\n",
    "        for char, counts in character_follow_count.items():\n",
    "            counter, total = counts\n",
    "            charList = []\n",
    "            probList = []\n",
    "            \n",
    "            for next_char, next_char_occur in counter.items():\n",
    "                charList.append(next_char)\n",
    "                probList.append(round( next_char_occur/total, 4) )\n",
    "            \n",
    "            char_probs[char] = (charList, probList)\n",
    "\n",
    "        #FINALLY, GENERATE A WEIGHTED SAMPLER FROM THE CHAR LIST AND PROB LIST FOR EACH CHARACTER\n",
    "        character_model = {}\n",
    "        for char, lists in char_probs.items():\n",
    "            character_model[char] = self.weighted_sampler(lists[0], lists[1])\n",
    "        self.character_model = character_model\n",
    "        \n",
    "    def weighted_sampler(self, charList, probList):\n",
    "        totals = []\n",
    "        for p in probList:\n",
    "            totals.append(p + totals[-1] if totals else p)\n",
    "        return lambda: charList[bisect.bisect(totals, random.uniform(0, totals[-1]))]\n",
    "\n",
    "    #SELECT A CHARACTER TO FOLLOW THE GIVEN CHARACTER X\n",
    "    def next_character(self, x):\n",
    "        return self.character_model[x]()\n",
    "    \n",
    "    def generate_character_order(self):\n",
    "        #RANDOMLY CHOOSE FIRST CHARACTER -- WILL EDIT LATER TO BE WITH <t>\n",
    "        first = random.choice(self.characters)\n",
    "        order = [first]\n",
    "        \n",
    "        for i in range(self.numberLines - 1):\n",
    "            order.append(self.next_character(order[i]))\n",
    "        self.ordering = order\n",
    "    \n",
    "    def get_ordering(self):\n",
    "        return self.ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateScript(filename, interactionCount, ngram, maxTokens=25, topk=False, minimumOccurCount=1):\n",
    "    ch = character_model(filename, numberLines=interactionCount, minimumOccurCount=minimumOccurCount)\n",
    "    scriptOrder = ch.get_ordering()\n",
    "    sentence_models = {}\n",
    "    for character in scriptOrder:\n",
    "        if character not in sentence_models:\n",
    "            sentence_models[character] = sentence_model(filename, character, ngram=ngram)\n",
    "        print(character.upper() + \" -- \" )\n",
    "        print( \"\\t\" + sentence_models[character].create_sentence(maxTokens=maxTokens, topk=topk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLANKTON -- \n",
      "\t i win i game bow around cant torture basss i lets yes\n",
      "SPONGEBOB -- \n",
      "\t hey. jelly with bet song em hi friend tall help rotten\n",
      "MR. KRABS -- \n",
      "\t hang on private speak eye he wont be go spot hang ah\n",
      "PLANKTON -- \n",
      "\t our special today now just when friendship yoo getting finally popcorn yee\n",
      "SPONGEBOB -- \n",
      "\t last one won accept krabs far mean thought see like have back\n",
      "PLANKTON -- \n",
      "\t no not should realize enough just patty what hoo customer basss oh\n",
      "KAREN -- \n",
      "\t sounds like see when take fun take leave do this see revenge\n",
      "PLANKTON -- \n",
      "\t wh what win when much was krusty thermonuclear much saving loser special\n",
      "SPONGEBOB -- \n",
      "\t so i zone out.\n",
      "MR. KRABS -- \n",
      "\t look at will reach at recipe hes mutiny suit out go him\n",
      "NARRATOR -- \n",
      "\t everyone is its their is perhaps squarepants at one enjoying can to\n",
      "PLANKTON -- \n",
      "\t i tricked enough was spongebuddy appetite help inside induce bubble some play\n",
      "KAREN -- \n",
      "\t sounds like cant then going at sounds at krabby time is are\n",
      "PLANKTON -- \n",
      "\t aah. well finally yoo chased customer thats then customer whole part\n",
      "CLAMS -- \n",
      "\t â™ª. the.\n"
     ]
    }
   ],
   "source": [
    "generateScript(\"./data/season1_22.txt\", 15, 4, 10, minimumOccurCount=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentence_model:\n",
    "    def __init__(self, filename, character, ngram=2, alpha=.1):\n",
    "        self.ngram = ngram\n",
    "        self.ngramCounts = [i for i in range(ngram - 1, ngram + 1)]\n",
    "        self.filename = filename\n",
    "        self.character = character\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.textContent = self.clean_input_document()\n",
    "        self.textDict = list( set(word for line in self.textContent for word in line.split(\" \") if word) )\n",
    "        self.textDictSize = len(self.textDict)\n",
    "        self.wordCounts = self.count_words(self.textContent)\n",
    "        self.word2index = {w: i for i, w in enumerate(self.wordCounts[self.ngramCounts[0]].keys())}\n",
    "        self.probabilities = self.get_probability_matrix()\n",
    "\n",
    "        \n",
    "    def clean_input_document(self):\n",
    "        with open(self.filename) as f:\n",
    "            contents = f.read().lower()\n",
    "        translate = \"?:!-\\n\"\n",
    "        replace = \"...  \"\n",
    "        delete = \",;_()\\\"\"\n",
    "        table = contents.maketrans(translate, replace, delete)\n",
    "\n",
    "        #SPLIT SO EACH ELEMENT IS EITHER A CHARACTER OR ACTION\n",
    "        contents = contents.translate(table).strip().split(\"}} {{l|\")\n",
    "        contents[0] = contents[0].replace(\"{{l|\",\"\")\n",
    "        contents[-1] = contents[-1][:contents[-1].index('}')] \n",
    "\n",
    "        #REMOVE ACTIONS FROM CHARACTER LINES\n",
    "        for i, line in enumerate(contents):\n",
    "            if line[0:3] == \"''[\":\n",
    "                contents[i] = \"action|\" + line[3:-3]\n",
    "                continue\n",
    "            while \"''[\" in line:\n",
    "                line = line[:line.index(\"''[\")] + line[line.index(\"]''\") + 3:]\n",
    "            contents[i] = line\n",
    "\n",
    "        #KEEP ONLY IMPORTANT CHARACTERS TEXT\n",
    "        \n",
    "        relevant_char = self.character\n",
    "        char_lines = []\n",
    "        for line in contents:\n",
    "            line = line.split(\"|\")\n",
    "            if line[0] != relevant_char:\n",
    "                continue\n",
    "\n",
    "            #ADD SENTENCE TAGS\n",
    "            text = line[1].strip().replace(\"'\", \"\").replace('[',\"\").replace(']', \"\").split(\".\")\n",
    "            text = [\"<s> \" + t.strip() + \" </s>\" for t in text if t]\n",
    "            char_lines.extend(text)\n",
    "        return char_lines\n",
    "            \n",
    "    def count_words(self, contents):\n",
    "        \"\"\"Iterate through the contents and gather the counts of words\"\"\"\n",
    "        wordCounts = {}\n",
    "        for i in self.ngramCounts:\n",
    "            if i == 0: # want the default to be the size of the corpus\n",
    "                total = 0\n",
    "                for line in contents:\n",
    "                    words = line.split(\" \")\n",
    "                    words = [ w.strip() for w in words if w] #remove nulls\n",
    "                    for word in words:\n",
    "                        if word:\n",
    "                            total += 1\n",
    "                wordCounts[i] = defaultdict(lambda: total)\n",
    "                continue\n",
    "            else:\n",
    "                counts = defaultdict(lambda: 0)\n",
    "            for line in contents:\n",
    "                words = line.split(\" \")\n",
    "                words = [ w.strip() for w in words if w] #remove nulls\n",
    "                for k, word in enumerate(words): \n",
    "                    if k < (i-1) or not word:\n",
    "                        continue\n",
    "                    key = \"\"\n",
    "                    for j in range(k-i+1, k+1):\n",
    "                        key += words[j] + \" \"\n",
    "                    counts[key.strip()] += 1\n",
    "            wordCounts[i] = counts\n",
    "        return wordCounts\n",
    "\n",
    "    def model(self, x):\n",
    "        return (self.wordCounts[self.ngramCounts[1]][x[0]] + self.alpha) / \\\n",
    "                    ( self.wordCounts[self.ngramCounts[0]][x[1]] + self.textDictSize*self.alpha )\n",
    "    \n",
    "    def get_probability_matrix(self):\n",
    "        probabilities = []\n",
    "        for wordA in self.wordCounts[self.ngramCounts[0]].keys():\n",
    "            line = []\n",
    "            for wordB in self.textDict:\n",
    "                line.append(self.model([wordA + \" \" + wordB, wordA]))\n",
    "            probabilities.append(line)\n",
    "        return np.array(probabilities)\n",
    "    \n",
    "    def get_starting_word(self):\n",
    "        potential = []\n",
    "        for phrase in self.wordCounts[self.ngramCounts[0]].keys():\n",
    "            if phrase[:3] == \"<s>\":\n",
    "                potential.append(phrase)\n",
    "        return random.choice(potential)\n",
    "\n",
    "    def generate_sample_top_k(self, lm, index2word):\n",
    "        \"\"\" Taken from - http://veredshwartz.blogspot.com/2019/08/text-generation.html\n",
    "        Generates a string, sample a word from the top k probable words in the distribution at each time step.\n",
    "        :param lm - the language model\n",
    "        :param index2word - a mapping from the index of a word in the vocabulary to the word itself\n",
    "        :param k - how many words to keep in the distribution \"\"\" \n",
    "\n",
    "        generated_sentence = self.get_starting_word()\n",
    "        curr_token = None\n",
    "        generated_tokens = 0\n",
    "\n",
    "        while curr_token != '</s>' and generated_tokens < self.maxTokens:\n",
    "            #NEED TO CHOOSE A ROW ELEMENT -- LAST n-1 WORDS OF SENTENCE\n",
    "            gen_list = generated_sentence.split()[-self.ngramCounts[0]:]\n",
    "            gen_row = \" \".join(gen_list)\n",
    "  \n",
    "            curr_distribution = lm(gen_row)  # vector of probabilities\n",
    "            sorted_by_probability = np.argsort(curr_distribution) # sort by probability\n",
    "            top_k = sorted_by_probability[-(self.k+1):] # keep the top k words\n",
    "\n",
    "            k_index2Word = [] #grab the top k words associated with top_k probabilities\n",
    "            for index in top_k:\n",
    "                k_index2Word.append(index2word[index])\n",
    "\n",
    "            # normalize to make it a probability distribution again\n",
    "            top_k = top_k / np.sum(top_k)\n",
    "\n",
    "            selected_index = np.random.choice(range(len(k_index2Word)), p=top_k)\n",
    "            curr_token = k_index2Word[int(selected_index)]\n",
    "            generated_sentence += ' ' + curr_token   \n",
    "            generated_tokens += 1\n",
    "\n",
    "        return generated_sentence\n",
    "\n",
    "\n",
    "    def generate_sample(self, lm, index2word):\n",
    "        \"\"\" Taken from http://veredshwartz.blogspot.com/2019/08/text-generation.html\n",
    "        Generates a string, sample a word from the distribution at each time step.\n",
    "        :param lm - the language model\n",
    "        :param index2word - a mapping from the index of a word in the vocabulary to the word itself \"\"\" \n",
    "        \n",
    "        generated_sentence = self.get_starting_word()\n",
    "        generated_tokens = 0\n",
    "        curr_token = None\n",
    "\n",
    "        while curr_token != '</s>' and generated_tokens < self.maxTokens:\n",
    "            #NEED TO CHOOSE A ROW ELEMENT -- LAST n-1 WORDS OF SENTENCE\n",
    "            gen_list = generated_sentence.split()[-self.ngramCounts[0]:]\n",
    "            gen_row = \" \".join(gen_list)\n",
    "\n",
    "            curr_distribution = lm(gen_row)  # vector of probabilities\n",
    "            curr_distribution /= np.sum(curr_distribution)\n",
    "\n",
    "            selected_index = np.random.choice(range(len(index2word)), p=curr_distribution)\n",
    "            curr_token = index2word[int(selected_index)]\n",
    "            generated_sentence += ' ' + curr_token\n",
    "            generated_tokens += 1\n",
    "\n",
    "        return generated_sentence\n",
    "\n",
    "    def create_sentence(self, topk=False, maxTokens=25, k=2):\n",
    "        self.k = k\n",
    "        self.maxTokens = maxTokens\n",
    "        lm = lambda s: self.probabilities[self.word2index.get(s, -1), :]\n",
    "        \n",
    "        if topk:\n",
    "            sentence =  self.generate_sample_top_k(lm, self.textDict)\n",
    "        else:\n",
    "            sentence =  self.generate_sample(lm, self.textDict)\n",
    "            \n",
    "        return sentence.replace(\" </s>\", \".\").replace(\"<s>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
